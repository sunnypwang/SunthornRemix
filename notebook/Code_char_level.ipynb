{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import codecs\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunthornphu_path = listdir(\"db/sunthornphu/\")\n",
    "other_path = listdir(\"db/other/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sunthornphu = []\n",
    "for i in sunthornphu_path:\n",
    "    with codecs.open(\"db/sunthornphu/\"+str(i), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_sunthornphu.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(data_sunthornphu)):\n",
    "    data_sunthornphu[i] = data_sunthornphu[i][0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_other = []\n",
    "for i in other_path:\n",
    "    with codecs.open(\"db/other/\"+str(i), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_other.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(data_other)):\n",
    "    data_other[i] = data_other[i][0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add this\n",
    "for i in range(1,len(data_sunthornphu)//len(data_other)):\n",
    "    data_sunthornphu = data_sunthornphu+data_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Character Level #############\n",
    "\n",
    "word_to_idx = {\"<PAD>\":0, \"<s>\":1, \"</s>\":2}\n",
    "idx_to_word = {0:\"<PAD>\", 1:\"<s>\", 2:\"</s>\"}\n",
    "\n",
    "i = 3\n",
    "maxlen = 0\n",
    "for sentence_poem in data_sunthornphu:\n",
    "    if(len(sentence_poem) > maxlen):\n",
    "        maxlen = len(sentence_poem)\n",
    "    for word_poem in sentence_poem:\n",
    "        if(word_poem == \"\\r\" or word_poem == \"\\n\"):\n",
    "            break\n",
    "        if(word_poem not in word_to_idx.keys()):\n",
    "            word_to_idx[word_poem] = i\n",
    "            idx_to_word[i] = word_poem\n",
    "            i+=1\n",
    "\n",
    "vocab_size = len(idx_to_word)\n",
    "##############################################\n",
    "\n",
    "########## Word Level #############\n",
    "# word_to_idx = {\"<PAD>\":0, \"<s>\":1, \"</s>\":2, \"UNK\":3}\n",
    "# idx_to_word = {0:\"<PAD>\", 1:\"<s>\", 2:\"</s>\", 3:\"UNK\"}\n",
    "# maxlen = 0\n",
    "# ########## Word Count ############\n",
    "# word_dict_count = {}\n",
    "# for i in range(len(data_sunthornphu)):\n",
    "#     tmp = []\n",
    "#     for j in data_sunthornphu[i].split(\"/\"):\n",
    "#         tmp.extend(word_tokenize(j, engine=\"newmm\", whitespaces=False))\n",
    "#         tmp.extend(\"/\")\n",
    "#     data_sunthornphu[i] = tmp[0:-1]\n",
    "#     if( len(data_sunthornphu[i]) > maxlen):\n",
    "#         maxlen = len(data_sunthornphu[i])\n",
    "#     for word_poem in data_sunthornphu[i]:\n",
    "#         if(word_poem not in word_dict_count.keys()):\n",
    "#             word_dict_count[word_poem] = 1\n",
    "#         else:\n",
    "#             word_dict_count[word_poem]+=1\n",
    "# print(\"Finished Counting\")\n",
    "            \n",
    "# word_count = 3\n",
    "# for i in range(len(data_sunthornphu)):\n",
    "#     for word_poem in data_sunthornphu[i]:\n",
    "#         if((word_poem not in word_to_idx.keys()) and word_dict_count[word_poem]>5):\n",
    "#             word_to_idx[word_poem] = word_count\n",
    "#             idx_to_word[word_count] = word_poem\n",
    "#             word_count+=1\n",
    "            \n",
    "# vocab_size = len(idx_to_word)\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.Series(list(word_dict_count.values())).value_counts()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "data_size = len(data_sunthornphu)\n",
    "\n",
    "################# Character Level ################\n",
    "for sentence_poem in data_sunthornphu:\n",
    "    tmp_X = [1]\n",
    "    tmp_Y = []\n",
    "    for char_poem in sentence_poem:\n",
    "        tmp_X.append(word_to_idx[char_poem])\n",
    "        tmp_Y.append(word_to_idx[char_poem])\n",
    "    X.append(tmp_X)\n",
    "    tmp_Y.append(2)\n",
    "    Y.append(tmp_Y)\n",
    "###################################################\n",
    "\n",
    "################# Word Level ################\n",
    "# for sentence_poem in data_sunthornphu:\n",
    "#     tmp_X = [1]\n",
    "#     tmp_Y = [1]\n",
    "#     for word_poem in sentence_poem:\n",
    "#         if(word_dict_count[word_poem]<=5):\n",
    "#             tmp_X.append(word_to_idx['UNK'])\n",
    "#             tmp_Y.append(word_to_idx['UNK'])\n",
    "#             continue\n",
    "#         tmp_X.append(word_to_idx[word_poem])\n",
    "#         tmp_Y.append(word_to_idx[word_poem])\n",
    "#     tmp_X.append(2)\n",
    "#     tmp_Y.append(2)\n",
    "#     X.append(tmp_X)\n",
    "#     Y.append(tmp_Y)\n",
    "###################################################\n",
    "\n",
    "# X = pad_sequences(np.array(X[0:20000]), maxlen=maxlen)\n",
    "# # X = to_categorical(X,vocab_size)\n",
    "# # X = X.reshape(10000, maxlen , vocab_size)\n",
    "\n",
    "# Y = pad_sequences(np.array(Y[0:20000]), maxlen=maxlen)\n",
    "# Y = to_categorical(Y,vocab_size)\n",
    "# Y = Y.reshape(20000, maxlen , vocab_size)\n",
    "\n",
    "# print(\"X shape : \",X.shape)\n",
    "# print(\"Y shape : \",Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.activations import softmax\n",
    "# def softMaxAxis1(x):\n",
    "#     return softmax(x,axis=1)\n",
    "\n",
    "# Tx = X.shape[1]\n",
    "# Ty = Y.shape[1]\n",
    "# # Ty = 40\n",
    "# m = data_size\n",
    "\n",
    "# #These are global variables (shared layers)\n",
    "# repeator = RepeatVector(Tx)\n",
    "# concatenator = Concatenate(axis=-1)\n",
    "# #Attention function###\n",
    "# fattn_1 = Dense(256, activation = \"tanh\")\n",
    "# fattn_2 = Dense(16, activation = \"relu\")\n",
    "# ###\n",
    "# activator = Activation(softMaxAxis1, name='attention_scores') \n",
    "# dotor = Dot(axes = 1)\n",
    "\n",
    "# def one_step_attention(a, s_prev):\n",
    "\n",
    "#     # Repeat the decoder hidden state to concat with encoder hidden states\n",
    "#     s_prev = repeator(s_prev)\n",
    "#     concat = concatenator([a,s_prev])\n",
    "#     # attention function\n",
    "#     e = fattn_1(concat)\n",
    "#     energies =fattn_2(e)\n",
    "#     # calculate attention_scores (softmax)\n",
    "#     attention_scores = activator(energies)\n",
    "#     #calculate a context vector\n",
    "#     context = dotor([attention_scores,a])\n",
    "\n",
    "#     return context\n",
    "\n",
    "### Shared Layer\n",
    "# n_h = 64 #hidden dimensions for encoder \n",
    "# n_s = 128 #hidden dimensions for decoder\n",
    "# decoder_LSTM_cell = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n",
    "# output_layer = Dense(vocab_size, activation=\"softmax\") #softmax output layer\n",
    "# embed_layer = Embedding(len(word_to_idx), n_h, input_length=maxlen,mask_zero=True)\n",
    "# bi_LSTM = Bidirectional(LSTM(n_h, return_sequences=True))\n",
    "\n",
    "# out_s1 = Dense(256,activation=\"relu\")\n",
    "# out_s2 = Dense(256,activation=\"relu\")\n",
    "\n",
    "# def build_model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n",
    "\n",
    "#     # Define the input of your model\n",
    "#     X = Input(shape=(Tx,))\n",
    "#     # Define hidden state and cell state for decoder_LSTM_Cell\n",
    "#     s0 = Input(shape=(n_s,), name='s0')\n",
    "#     c0 = Input(shape=(n_s,), name='c0')\n",
    "#     s = s0\n",
    "#     c = c0\n",
    "    \n",
    "#     # Initialize empty list of outputs\n",
    "#     outputs = list()\n",
    "\n",
    "#     #Encoder Bi-LSTM\n",
    "#     h = embed_layer(X)\n",
    "# #     h = bi_LSTM(embed)\n",
    "  \n",
    "#     #Iterate for Ty steps (Decoding)\n",
    "#     for t in range(Ty):\n",
    "    \n",
    "#         #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
    "#         context = one_step_attention(h, s)\n",
    "       \n",
    "#         # Feed the context vector to the decoder LSTM cell\n",
    "#         s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
    "           \n",
    "#         # Pass the decoder hidden output to the output layer (softmax)\n",
    "#         o1 = out_s1(s)\n",
    "#         o2 = out_s2(o1)\n",
    "#         out = output_layer(o2)\n",
    "        \n",
    "#         # Append an output list with the current output\n",
    "#         outputs.append(out)\n",
    "    \n",
    "#     #Create model instance\n",
    "#     model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(Tx, Ty, n_h, n_s, vocab_size, vocab_size)\n",
    "# opt = Adam(lr= 0.005, decay = 0.0001)\n",
    "# model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['categorical_accuracy'])\n",
    "\n",
    "# m = 20000\n",
    "# s0 = np.zeros((m, n_s))\n",
    "# c0 = np.zeros((m, n_s))\n",
    "# outputs = list(Y.swapaxes(0,1))\n",
    "\n",
    "# model.fit([X, s0, c0], outputs, epochs=5, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_x = maxlen+5\n",
    "# in_x = 40\n",
    "n_h = 256\n",
    "def build_simple_model():\n",
    "    inp = Input(shape=(in_x,))\n",
    "    mod = Embedding(len(word_to_idx), n_h, input_length=in_x)(inp)\n",
    "    mod = Bidirectional(LSTM(n_h, return_sequences=True))(mod)\n",
    "    mod = BatchNormalization()(mod)\n",
    "    mod = Dropout(0.2)(mod)\n",
    "    mod = Flatten()(mod)\n",
    "    \n",
    "    mod = Dense(512)(mod)\n",
    "    mod = BatchNormalization()(mod)\n",
    "    mod = Activation('relu')(mod)\n",
    "    mod = Dropout(0.2)(mod)\n",
    "    \n",
    "    out = Dense(vocab_size, activation=\"softmax\")(mod)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr= 0.01, decay = 0.0001), metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 145, 256)          24576     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 145, 512)          1050624   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 145, 512)          2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 145, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 74240)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               38011392  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 96)                49248     \n",
      "=================================================================\n",
      "Total params: 39,139,936\n",
      "Trainable params: 39,137,888\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in X: ### เพิ่มเลขตัวนี้เท่าที่ Ram รับไหว\n",
    "    i = [0]*(in_x-1) + i \n",
    "    for j in range(in_x,len(i)):\n",
    "        y_train.append(i[j])\n",
    "        tmp = []\n",
    "        for k in range(in_x):\n",
    "            tmp.append(i[j-in_x+k])\n",
    "        X_train.append(np.array(tmp))\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "# y_train = to_categorical(y_train,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(in_x,), n_channels=1,\n",
    "                 n_classes=vocab_size, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        list_IDs_y = [self.labels[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp, list_IDs_y)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, list_IDs_y):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim,))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = ID\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = list_IDs_y[i]\n",
    "\n",
    "        return X, to_categorical(y, self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (in_x,),\n",
    "          'batch_size': 1024,\n",
    "          'n_classes': vocab_size,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(X_train, y_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"new_char_model_05.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, decay= 0.0000001), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=5, batch_size=2048, shuffle=True, verbose=1)\n",
    "# model.fit_generator(generator=training_generator, epochs=5, verbose=1, steps_per_epoch=len(X_train)//params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"8_5unk_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(text, word_to_index):\n",
    "    text = text.strip().split()\n",
    "#     print(text)\n",
    "    token_list = [word_to_index[x] for x in text]\n",
    "    return token_list\n",
    "\n",
    "\n",
    "current_text = \"<s> ฝ\"\n",
    "# current_text = \"<s> ก\"\n",
    "probs = []\n",
    "for _ in range(in_x):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    tmp = texts_to_sequences(current_text, word_to_idx)\n",
    "\n",
    "    tmp = pad_sequences([tmp], maxlen=in_x, value=0.0)\n",
    "\n",
    "    output_word = model.predict(tmp)\n",
    "    probs.append(output_word.max())\n",
    "    output_word = np.argmax(output_word)\n",
    "    output_word = idx_to_word[output_word]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    current_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ฝ ่ า ย พ ร ะ อ ง ค ์ ท ร ง ศ ร ี ส ุ ว ร ร ณ / เ ห ็ น เ ห ็ น เ ห ็ น เ ห ็ น เ ป ็ น ส ุ ข า ร ณ ์ / จ ึ ง จ ะ ไ ด ้ เ ห ็ น เ ป ็ น เ ป ็ น ท ี ่ ร ั ก / เ ห ็ น จ ะ ไ ด ้ เ ห ็ น เ ป ็ น เ ป ็ น เ ป ็ น ท ี ่ น ี ้ น ี ้ น ี ้ น ี ้ อ ย ม า ย                                            '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(score_list, length, normalized=False):\n",
    "\n",
    "    seq_score = 0\n",
    "    tmp_length = min(length,len(score_list))\n",
    "    for i in range(length):\n",
    "\n",
    "        seq_score+=np.log(score_list[i] + 0.00000001)\n",
    "    \n",
    "    if(normalized):\n",
    "        seq_score/=length\n",
    "    \n",
    "    return np.exp(seq_score)\n",
    "\n",
    "def beam_search_decode(seed_text, max_gen_len, model, word_to_index, index_to_word, max_sequence_len, beam_size, normalized=False):\n",
    "\n",
    "    beams = [[seed_text, [], 0]]\n",
    "\n",
    "    for _ in range(max_gen_len):\n",
    "        \n",
    "        tmp_beams = beams.copy()\n",
    "        beams = []\n",
    "        for beam in tmp_beams:\n",
    "            if(beam[0].split()[-1] == \"</s>\"):\n",
    "                beams.append(beam)\n",
    "                continue\n",
    "                \n",
    "            tmp = texts_to_sequences(beam[0], word_to_index)\n",
    "            tmp = pad_sequences([tmp], maxlen=max_sequence_len, value=0.0)\n",
    "            \n",
    "            output_word = model.predict(tmp).reshape(-1)\n",
    "            top_beam_idx = np.argsort(output_word)[-1*beam_size:]\n",
    "            top_beam_values = [output_word[i] for i in top_beam_idx]\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                tmp_beam_0 = beam[0] + \" \" + index_to_word[top_beam_idx[i]]\n",
    "                tmp_beam_1 = beam[1] + [top_beam_values[i]]\n",
    "                tmp_beam_2 = beam[2] + 1\n",
    "\n",
    "                beams.append([tmp_beam_0, tmp_beam_1, tmp_beam_2])\n",
    "                \n",
    "        b_cal = []\n",
    "        for beam in beams:\n",
    "            b_cal.append(cal_score(beam[1], beam[2], normalized))\n",
    "        top_beam_idx = np.argsort(b_cal)[-1*beam_size:]\n",
    "        beams = [beams[x] for x in top_beam_idx]\n",
    "    \n",
    "    return beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Normalized-\n",
      "['<s> ฝ ่ า ย พ ร ะ อ ง ค ์ ท ร ง ศ ร ี ส ุ ว ร ร ณ ร ั ต น ์ / เ ห ม ื อ น อ ย ่ า ง เ ร ื ่ อ ง ร า ว ก ร ะ ห ว ั ด ห ว ั ง / พ ร ะ ส ั ่ ง เ ส ร ็ จ ส ำ เ ร ็ จ เ ส ร ็ จ จ า ร ย ์ / เ ส ี ย ง ค ร ั ้ น ค ร ั ่ ง พ ร า ห ม ณ ์ พ ร ะ ท ั ย ธ า น ี ่ า ล า ว ่ า น ี                              ', [0.93416196, 0.9980757, 0.98686165, 0.29290777, 0.8553386, 0.9698036, 0.2672103, 0.822596, 0.94945645, 0.9976828, 0.4053514, 0.8485812, 0.98997974, 0.17646337, 0.39683193, 0.63135934, 0.68561435, 0.684838, 0.8893625, 0.9826649, 0.9837446, 0.99355584, 0.153647, 0.8024082, 0.28795826, 0.46122706, 0.9472398, 0.9879588, 0.15510085, 0.20685707, 0.22779466, 0.9677416, 0.9918104, 0.97090155, 0.09490545, 0.26751858, 0.5331597, 0.94628763, 0.694505, 0.12772065, 0.1950263, 0.6563752, 0.19596805, 0.99757427, 0.9339443, 0.16701752, 0.3233393, 0.48177928, 0.11700154, 0.1822179, 0.44190237, 0.2560029, 0.2711085, 0.3845862, 0.43398002, 0.20230475, 0.59899575, 0.36430773, 0.47412091, 0.9116616, 0.092856556, 0.5184936, 0.8837474, 0.09487375, 0.39929315, 0.61733365, 0.9628897, 0.28731015, 0.35572064, 0.47173858, 0.9101523, 0.98144066, 0.2537993, 0.2868345, 0.31925315, 0.51737976, 0.9566805, 0.9146513, 0.2335058, 0.35705185, 0.49877137, 0.944814, 0.9918399, 0.19590144, 0.34208858, 0.43798044, 0.32007056, 0.77984774, 0.8622211, 0.1226148, 0.20741186, 0.2238452, 0.9833128, 0.5431796, 0.1576186, 0.5748577, 0.30210733, 0.620854, 0.443804, 0.21580262, 0.7692302, 0.22010024, 0.5301756, 0.69112563, 0.13745318, 0.7007442, 0.55008274, 0.31333807, 0.6072352, 0.8195334, 0.9981651, 0.13256168, 0.49187353, 0.9031925, 0.15714464, 0.5902507, 0.90799963, 0.11973848, 0.677363, 0.44190916, 0.6861984, 0.5306899, 0.3435132, 0.22376654, 0.5804522, 0.19464058, 0.35654137, 0.9852409, 0.2815534, 0.50714856, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854, 0.5575854], 145]\n",
      "\n",
      "____\n",
      "-Normalized-\n"
     ]
    }
   ],
   "source": [
    "sample_seeds = [\"<s> ฝ\", \"<s> เ\",]\n",
    "# sample_seeds = [ \"america\"]\n",
    "\n",
    "for seed in sample_seeds:\n",
    "    \n",
    "#     print(\"-Greedy-\")\n",
    "#     ans_greedy = greedy_decode(seed, 10, model, word_to_index, index_to_word, input_len)\n",
    "#     tmp_greedy = clean_output(ans_greedy[0],\"Eos\")\n",
    "#     print(tmp_greedy, cal_score(ans_greedy[1],min(len(tmp_greedy.split()), len(ans_greedy[0].split())-1), norm))\n",
    "#     print()\n",
    "    \n",
    "#     print(\"-Unnormalized-\")\n",
    "#     ans = beam_search_decode(seed, 10, model, word_to_index, index_to_word, input_len, 5, normalized=False)\n",
    "#     top_1 = ans[4]\n",
    "#     top_2 = ans[3]\n",
    "#     print(clean_output(top_1[0], 'eos').title(), cal_score(top_1[1], top_1[2], False))  \n",
    "#     print(clean_output(top_2[0], 'eos').title(), cal_score(top_2[1],top_2[2], False))\n",
    "#     print()\n",
    "    \n",
    "    print(\"-Normalized-\")\n",
    "    ans = beam_search_decode(seed, in_x, model, word_to_idx, idx_to_word, in_x, 2, normalized=True)\n",
    "    print(ans[1])\n",
    "    \n",
    "    print()\n",
    "    print(\"____\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
