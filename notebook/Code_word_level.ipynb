{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras\n",
    "import keras.backend as KXW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import codecs\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunthornphu_path = listdir(\"db/sunthornphu/\")\n",
    "other_path = listdir(\"db/other/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sunthornphu = []\n",
    "for i in sunthornphu_path:\n",
    "    with codecs.open(\"db/sunthornphu/\"+str(i), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_sunthornphu.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(data_sunthornphu)):\n",
    "    data_sunthornphu[i] = data_sunthornphu[i][0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_other = []\n",
    "for i in other_path:\n",
    "    with codecs.open(\"db/other/\"+str(i), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_other.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(data_other)):\n",
    "    data_other[i] = data_other[i][0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(data_sunthornphu)//len(data_other)):\n",
    "    data_sunthornphu = data_sunthornphu+data_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_sunthornphu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Counting\n"
     ]
    }
   ],
   "source": [
    "########## Character Level #############\n",
    "\n",
    "# word_to_idx = {\"<PAD>\":0, \"<s>\":1, \"</s>\":2}\n",
    "# idx_to_word = {0:\"<PAD>\", 1:\"<s>\", 2:\"</s>\"}\n",
    "\n",
    "# i = 3\n",
    "# maxlen = 0\n",
    "# for sentence_poem in data_sunthornphu:\n",
    "#     if(len(sentence_poem) > maxlen):\n",
    "#         maxlen = len(sentence_poem)\n",
    "#     for word_poem in sentence_poem:\n",
    "#         if(word_poem == \"\\r\" or word_poem == \"\\n\"):\n",
    "#             break\n",
    "#         if(word_poem not in word_to_idx.keys()):\n",
    "#             word_to_idx[word_poem] = i\n",
    "#             idx_to_word[i] = word_poem\n",
    "#             i+=1\n",
    "\n",
    "# vocab_size = len(idx_to_word)\n",
    "##############################################\n",
    "\n",
    "########## Word Level #############\n",
    "# word_to_idx = {\"<PAD>\":0, \"<s>\":1, \"</s>\":2, \"UNK\":3}\n",
    "# idx_to_word = {0:\"<PAD>\", 1:\"<s>\", 2:\"</s>\", 3:\"UNK\"}\n",
    "\n",
    "word_to_idx = {\"<PAD>\":0, \"<s>\":1, \"</s>\":2, \"UNK\":3}\n",
    "idx_to_word = {0:\"<PAD>\", 1:\"<s>\", 2:\"</s>\", 3:\"UNK\"}\n",
    "\n",
    "maxlen = 0\n",
    "########## Word Count ############\n",
    "word_dict_count = {}\n",
    "for i in range(len(data_sunthornphu)):\n",
    "    tmp = []\n",
    "    count_slash = 0\n",
    "    for j in data_sunthornphu[i].split(\"/\"):\n",
    "        tmp.extend(word_tokenize(j, engine=\"newmm\", whitespaces=False))\n",
    "        tmp.extend(\"/\")\n",
    "#         count_slash+=1\n",
    "#         if(count_slash == 2):\n",
    "#             tmp.extend(\"\\\\\")\n",
    "#         else:\n",
    "#             tmp.extend(\"/\")\n",
    "        \n",
    "    data_sunthornphu[i] = tmp[0:-1]\n",
    "    if( len(data_sunthornphu[i]) > maxlen):\n",
    "        maxlen = len(data_sunthornphu[i])\n",
    "    for word_poem in data_sunthornphu[i]:\n",
    "        if(word_poem not in word_dict_count.keys()):\n",
    "            word_dict_count[word_poem] = 1\n",
    "        else:\n",
    "            word_dict_count[word_poem]+=1\n",
    "print(\"Finished Counting\")\n",
    "            \n",
    "word_count = 3\n",
    "for i in range(len(data_sunthornphu)):\n",
    "    for word_poem in data_sunthornphu[i]:\n",
    "        if((word_poem not in word_to_idx.keys()) and word_dict_count[word_poem]>3):\n",
    "            word_to_idx[word_poem] = word_count\n",
    "            idx_to_word[word_count] = word_poem\n",
    "            word_count+=1\n",
    "            \n",
    "vocab_size = len(idx_to_word)\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump( word_to_idx, open( \"word_to_idx_sun.p\", \"wb\" ) )\n",
    "# pickle.dump( idx_to_word, open( \"idx_to_word_sun.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(list(word_dict_count.values())).value_counts()\n",
    "# word_to_idx[\"<s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "data_size = len(data_sunthornphu)\n",
    "\n",
    "################# Character Level ################\n",
    "# for sentence_poem in data_sunthornphu:\n",
    "#     tmp_X = [1]\n",
    "#     tmp_Y = []\n",
    "#     for char_poem in sentence_poem:\n",
    "#         tmp_X.append(word_to_idx[char_poem])\n",
    "#         tmp_Y.append(word_to_idx[char_poem])\n",
    "#     X.append(tmp_X)\n",
    "#     tmp_Y.append(2)\n",
    "#     Y.append(tmp_Y)\n",
    "###################################################\n",
    "\n",
    "################# Word Level ################\n",
    "for sentence_poem in data_sunthornphu:\n",
    "    tmp_X = [1]\n",
    "    tmp_Y = [1]\n",
    "    for word_poem in sentence_poem:\n",
    "        if(word_dict_count[word_poem]<=3):\n",
    "            tmp_X.append(word_to_idx['UNK'])\n",
    "            tmp_Y.append(word_to_idx['UNK'])\n",
    "            continue\n",
    "        tmp_X.append(word_to_idx[word_poem])\n",
    "        tmp_Y.append(word_to_idx[word_poem])\n",
    "    tmp_X.append(2)\n",
    "    tmp_Y.append(2)\n",
    "    X.append(tmp_X)\n",
    "    Y.append(tmp_Y)\n",
    "###################################################\n",
    "\n",
    "# X = pad_sequences(np.array(X[0:20000]), maxlen=maxlen)\n",
    "# # X = to_categorical(X,vocab_size)\n",
    "# # X = X.reshape(10000, maxlen , vocab_size)\n",
    "\n",
    "# Y = pad_sequences(np.array(Y[0:20000]), maxlen=maxlen)\n",
    "# Y = to_categorical(Y,vocab_size)\n",
    "# Y = Y.reshape(20000, maxlen , vocab_size)\n",
    "\n",
    "# print(\"X shape : \",X.shape)\n",
    "# print(\"Y shape : \",Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.activations import softmax\n",
    "# def softMaxAxis1(x):\n",
    "#     return softmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tx = X.shape[1]\n",
    "# Ty = Y.shape[1]\n",
    "# # Ty = 40\n",
    "# m = data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #These are global variables (shared layers)\n",
    "# repeator = RepeatVector(Tx)\n",
    "# concatenator = Concatenate(axis=-1)\n",
    "# #Attention function###\n",
    "# fattn_1 = Dense(256, activation = \"tanh\")\n",
    "# fattn_2 = Dense(16, activation = \"relu\")\n",
    "# ###\n",
    "# activator = Activation(softMaxAxis1, name='attention_scores') \n",
    "# dotor = Dot(axes = 1)\n",
    "\n",
    "# def one_step_attention(a, s_prev):\n",
    "\n",
    "#     # Repeat the decoder hidden state to concat with encoder hidden states\n",
    "#     s_prev = repeator(s_prev)\n",
    "#     concat = concatenator([a,s_prev])\n",
    "#     # attention function\n",
    "#     e = fattn_1(concat)\n",
    "#     energies =fattn_2(e)\n",
    "#     # calculate attention_scores (softmax)\n",
    "#     attention_scores = activator(energies)\n",
    "#     #calculate a context vector\n",
    "#     context = dotor([attention_scores,a])\n",
    "\n",
    "#     return context\n",
    "\n",
    "### Shared Layer\n",
    "# n_h = 64 #hidden dimensions for encoder \n",
    "# n_s = 128 #hidden dimensions for decoder\n",
    "# decoder_LSTM_cell = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n",
    "# output_layer = Dense(vocab_size, activation=\"softmax\") #softmax output layer\n",
    "# embed_layer = Embedding(len(word_to_idx), n_h, input_length=maxlen,mask_zero=True)\n",
    "# bi_LSTM = Bidirectional(LSTM(n_h, return_sequences=True))\n",
    "\n",
    "# out_s1 = Dense(256,activation=\"relu\")\n",
    "# out_s2 = Dense(256,activation=\"relu\")\n",
    "\n",
    "# def build_model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n",
    "\n",
    "#     # Define the input of your model\n",
    "#     X = Input(shape=(Tx,))\n",
    "#     # Define hidden state and cell state for decoder_LSTM_Cell\n",
    "#     s0 = Input(shape=(n_s,), name='s0')\n",
    "#     c0 = Input(shape=(n_s,), name='c0')\n",
    "#     s = s0\n",
    "#     c = c0\n",
    "    \n",
    "#     # Initialize empty list of outputs\n",
    "#     outputs = list()\n",
    "\n",
    "#     #Encoder Bi-LSTM\n",
    "#     h = embed_layer(X)\n",
    "# #     h = bi_LSTM(embed)\n",
    "  \n",
    "#     #Iterate for Ty steps (Decoding)\n",
    "#     for t in range(Ty):\n",
    "    \n",
    "#         #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
    "#         context = one_step_attention(h, s)\n",
    "       \n",
    "#         # Feed the context vector to the decoder LSTM cell\n",
    "#         s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
    "           \n",
    "#         # Pass the decoder hidden output to the output layer (softmax)\n",
    "#         o1 = out_s1(s)\n",
    "#         o2 = out_s2(o1)\n",
    "#         out = output_layer(o2)\n",
    "        \n",
    "#         # Append an output list with the current output\n",
    "#         outputs.append(out)\n",
    "    \n",
    "#     #Create model instance\n",
    "#     model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(Tx, Ty, n_h, n_s, vocab_size, vocab_size)\n",
    "# model = build_model_2()\n",
    "\n",
    "# opt = Adam(lr= 0.005, decay = 0.0001)\n",
    "# model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['categorical_accuracy'])\n",
    "\n",
    "# m = 20000\n",
    "# s0 = np.zeros((m, n_s))\n",
    "# c0 = np.zeros((m, n_s))\n",
    "# outputs = list(Y.swapaxes(0,1))\n",
    "\n",
    "# model.fit([X, s0, c0], outputs, epochs=5, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_x = maxlen+5\n",
    "# in_x = 40\n",
    "n_h = 256\n",
    "def build_simple_model():\n",
    "    inp = Input(shape=(in_x,))\n",
    "    mod = Embedding(len(word_to_idx), n_h, input_length=in_x)(inp)\n",
    "    mod = Bidirectional(LSTM(n_h, return_sequences=True))(mod)\n",
    "    mod = BatchNormalization()(mod)\n",
    "    mod = Dropout(0.2)(mod)\n",
    "    mod = Flatten()(mod)\n",
    "    \n",
    "    mod = Dense(512)(mod)\n",
    "    mod = BatchNormalization()(mod)\n",
    "    mod = Activation('relu')(mod)\n",
    "    mod = Dropout(0.2)(mod)\n",
    "    \n",
    "    ## For Test\n",
    "    mod = Dense(512)(mod)\n",
    "    mod = BatchNormalization()(mod)\n",
    "    mod = Activation('relu')(mod)\n",
    "    mod = Dropout(0.2)(mod)\n",
    "    \n",
    "    out = Dense(vocab_size, activation=\"softmax\")(mod)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr= 0.01, decay = 0.0001), metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 44, 256)           2914304   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 44, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 44, 512)           2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 22528)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               11534848  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11383)             5839479   \n",
      "=================================================================\n",
      "Total params: 21,608,055\n",
      "Trainable params: 21,604,983\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in X: ### เพิ่มเลขตัวนี้เท่าที่ Ram รับไหว\n",
    "    i = [0]*(in_x-1) + i \n",
    "    for j in range(in_x,len(i)):\n",
    "        y_train.append(i[j])\n",
    "        tmp = []\n",
    "        for k in range(in_x):\n",
    "            tmp.append(i[j-in_x+k])\n",
    "        X_train.append(np.array(tmp))\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "# y_train = to_categorical(y_train,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(in_x,), n_channels=1,\n",
    "                 n_classes=vocab_size, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        list_IDs_y = [self.labels[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp, list_IDs_y)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, list_IDs_y):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim,))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = ID\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = list_IDs_y[i]\n",
    "\n",
    "        return X, to_categorical(y, self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (in_x,),\n",
    "          'batch_size': 1024,\n",
    "          'n_classes': vocab_size,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(X_train, y_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"mix_word_weight.h5\")\n",
    "# model.load_weights(\"5_3unk_model.h5\")\n",
    "# model.load_weights(\"2_3unk_other_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, decay=0.0000001), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1676/1676 [==============================] - 1032s 616ms/step - loss: 2.5975 - categorical_accuracy: 0.5640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5309c84be0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=5, batch_size=2048, shuffle=True, verbose=1)\n",
    "model.fit_generator(generator=training_generator, epochs=1, verbose=1, steps_per_epoch=len(X_train)//params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mix_word.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(text, word_to_index):\n",
    "    text = text.strip().split()\n",
    "    token_list = [word_to_index[x] for x in text]\n",
    "    return token_list\n",
    "\n",
    "\n",
    "current_text = \"<s> ฝ่าย\"\n",
    "# current_text = \"<s> ก\"\n",
    "probs = []\n",
    "for _ in range(in_x):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    tmp = texts_to_sequences(current_text, word_to_idx)\n",
    "\n",
    "    tmp = pad_sequences([tmp], maxlen=in_x, value=0.0)\n",
    "\n",
    "    output_word = model.predict(tmp)\n",
    "    probs.append(output_word.max())\n",
    "    output_word = np.argmax(output_word)\n",
    "    output_word = idx_to_word[output_word]\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    current_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ฝ่าย พระ จอม นครินทร์ ปิ่น กษัตริย์ / โองการ ตรัส สั่ง ให้ พหล พลขันธ์ / ให้ เร่งรัด จัด พหล พล นิกาย / ให้ ตรวจตรา พล สกล ไกร </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(score_list, length, normalized=False):\n",
    "\n",
    "    seq_score = 0\n",
    "    tmp_length = min(length,len(score_list))\n",
    "    for i in range(length):\n",
    "\n",
    "        seq_score+=np.log(score_list[i] + 0.00000001)\n",
    "    \n",
    "    if(normalized):\n",
    "        seq_score/=length\n",
    "    \n",
    "    return np.exp(seq_score)\n",
    "\n",
    "def beam_search_decode(seed_text, max_gen_len, model, word_to_index, index_to_word, max_sequence_len, beam_size, normalized=False):\n",
    "\n",
    "    beams = [[seed_text, [], 0]]\n",
    "\n",
    "    for _ in range(max_gen_len):\n",
    "        \n",
    "        tmp_beams = beams.copy()\n",
    "        beams = []\n",
    "        for beam in tmp_beams:\n",
    "            if(beam[0].split()[-1] == \"</s>\"):\n",
    "                beams.append(beam)\n",
    "                continue\n",
    "                \n",
    "            tmp = texts_to_sequences(beam[0], word_to_index)\n",
    "            tmp = pad_sequences([tmp], maxlen=max_sequence_len, value=0.0)\n",
    "            \n",
    "            output_word = model.predict(tmp).reshape(-1)\n",
    "            top_beam_idx = np.argsort(output_word)[-1*beam_size:]\n",
    "            top_beam_values = [output_word[i] for i in top_beam_idx]\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                tmp_beam_0 = beam[0] + \" \" + index_to_word[top_beam_idx[i]]\n",
    "                tmp_beam_1 = beam[1] + [top_beam_values[i]]\n",
    "                tmp_beam_2 = beam[2] + 1\n",
    "\n",
    "                beams.append([tmp_beam_0, tmp_beam_1, tmp_beam_2])\n",
    "                \n",
    "        b_cal = []\n",
    "        for beam in beams:\n",
    "            b_cal.append(cal_score(beam[1], beam[2], normalized))\n",
    "        top_beam_idx = np.argsort(b_cal)[-1*beam_size:]\n",
    "        beams = [beams[x] for x in top_beam_idx]\n",
    "    \n",
    "    return beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Normalized-\n",
      "['<s> ฝ่าย พระ จอม มัง คลา นรา ราช / กับ ครู บาทหลวง สม อารมณ์ ถวิล / จึง ตรัส สั่ง เส ว กา พฤฒา มา ตย์ / ให้ หมาย บาด พวก พหล พล พล ฯ </s>', [0.23030378, 0.432929, 0.19835511, 0.99977237, 0.8906924, 0.9790567, 0.99993813, 0.19662936, 0.47052595, 0.9629918, 0.06568807, 0.8065423, 0.32461634, 0.99924386, 0.11659378, 0.20081393, 0.41309166, 0.3804587, 0.9849799, 0.7157101, 0.2234941, 0.8169417, 0.99802005, 0.9920574, 0.1788701, 0.05491309, 0.743909, 0.30383262, 0.10857411, 0.94221574, 0.39360577, 0.40966803, 0.99999666], 33]\n",
      "\n",
      "____\n",
      "-Normalized-\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-a44c1f4aab54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Normalized-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-195-8a4b3dc9ecab>\u001b[0m in \u001b[0;36mbeam_search_decode\u001b[0;34m(seed_text, max_gen_len, model, word_to_index, index_to_word, max_sequence_len, beam_size, normalized)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mtop_beam_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop_beam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_beam_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.env/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_seeds = [\"<s> ฝ่าย\", \"<s> ไม่\", \"<s> ทั้ง\", \"<s> ถึง\", \"<s> หา\"]\n",
    "# sample_seeds = [ \"america\"]\n",
    "\n",
    "for seed in sample_seeds:\n",
    "    \n",
    "#     print(\"-Greedy-\")\n",
    "#     ans_greedy = greedy_decode(seed, 10, model, word_to_index, index_to_word, input_len)\n",
    "#     tmp_greedy = clean_output(ans_greedy[0],\"Eos\")\n",
    "#     print(tmp_greedy, cal_score(ans_greedy[1],min(len(tmp_greedy.split()), len(ans_greedy[0].split())-1), norm))\n",
    "#     print()\n",
    "    \n",
    "#     print(\"-Unnormalized-\")\n",
    "#     ans = beam_search_decode(seed, 10, model, word_to_index, index_to_word, input_len, 5, normalized=False)\n",
    "#     top_1 = ans[4]\n",
    "#     top_2 = ans[3]\n",
    "#     print(clean_output(top_1[0], 'eos').title(), cal_score(top_1[1], top_1[2], False))  \n",
    "#     print(clean_output(top_2[0], 'eos').title(), cal_score(top_2[1],top_2[2], False))\n",
    "#     print()\n",
    "    \n",
    "    print(\"-Normalized-\")\n",
    "    ans = beam_search_decode(seed, in_x, model, word_to_idx, idx_to_word, in_x, 10, normalized=True)\n",
    "    print(ans[-1])\n",
    "    \n",
    "    print()\n",
    "    print(\"____\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
